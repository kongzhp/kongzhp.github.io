---
layout:     post
title:      "Cassandra使用笔记（1）"
subtitle:   "basic concepts"
date:       2015-08-14 17:35:00
author:     "Pete Kong"
header-img: "img/post-bg-02.jpg"
---

# Introduce
--------------------

Cassandra 是一个水平扩展性很强、高可用的nosql数据库，其他介绍就不说了，可以在官网上找到。而本系列的笔记将介绍cassandra的架构（虽然很多只是翻译）开始，在介绍其使用方法。

# Understanding Architecture
-------------------------

## 术语

* Node

	存储数据的节点
* Data center

	相关nodes的集合，一个data center可能有一个物理或者虚拟的DC组成。不同的workloads?应该使用分离的DC。Replication也是基于DC层面。通过使用分离的DC，可以隔离不同workload的对cassandra事务的影响，并且可以保持相关的请求发送到同一个DC上，从而保证低latency。通过replication的配置，数据可以写到多个data center。但是，data centers不能分散于不同的物理位置。

* Cluster

	一个cluster包含了一个或多个data centers，它可以跨越多个物理位置。
* Commit log

	所有数据会先写到commit log去持久化。当所有这些数据flush到SSTables后，commit log就会被replica、删除或再循环。
* Table

	有序列的集合。每一行包含若干列，并且有一个主键，键的第一部分是一个列的名字。
* SSTable

	SSTable: sorted string table，它是一个不可变的数据文件，cassadra定期把memtables写到这个文件里。它是一个append only的文件，并且每一个cassandra table都维护一份。

## 架构简述

cassandra是使用多nodes处理大数据流并且没有单点故障而设计的系统。cassandra节点每秒都会跨cluster地交换信息。每次持续的写操作都会写commit log来持久化数据。然后数据会被建立索引并写到一个内存数据结构上：memtable。当memtable满了，数据将会写到SSTables上。所有写操作都是自动分区，并且在cluster里自动做复制replica。

cassandra是一个面向行的数据库，它允许任何已授权用户连接节点并使用CQL访问数据。CQL的语法类似SQL。一般地，一个cluster对每一个application有一个keyspace，

client可以向cluster里任何一个node进行读和写。当一个client连接了一个node并向它发出读写请求，这个node会作为一个coordinator（协调者）或者一个代理一样把请求转发给合适的node（取决于cluster的配置）。

## 配置cassandra的关键components

* Groosip

	它是一个节点之间点对点的通讯协议，它用于发现和共享节点的位置和状态信息。每当节点重启的时候，gossip信息就会被持久化。
* Partitioner

	parttitioner用于决定把数据分发到哪个节点，以及决定数据的第一个replica存放到哪里。一般地，partitioner是一个哈希函数，这哈希函数用于计算一个partition key的token，而每一行数据是用partition key来唯一标识。并通过这个token值来分发到cluster的节点中。默认的partioning策略是Murmur3Partitioner，你必须为每一个node配置一个num_toknes值。tokens的数量取决于硬件的能力。
* Replication factor

	repliaction factor是指在cluster里数据的replica数。如果replication factor是1，那么在一个节点中，每一行数据就会只有一个replica。如果是2，那么就有两个replica，而且分布在不同的节点上。你需要为每个DC设置replication factor,而且大于1，但不大于cluster的节点数。
* Snitch

	snitch是在racks(机架)和data center里定义的分组，指定组上使用的replication策略。所有snitches使用一个动态snitch层，它用于监控性能，当处理读请求时，它会选择一个性能最好的节点来读取replica。
	默认的SimpleSnitch不会识别data center或者rack的信息，当只有单个data center或者单zone的时候使用。在生产环境上推荐使用 GossipingPropertyFileSnitch，它定义了一个节点的data center和rack的信息，并且使用groosip协议收集其他节点的信息。

## 节点间通讯（gossip）

Gossip是节点间定期交换状态信息的点对点的通讯协议。节点会把自己的和它所知道其他节点的信息通过gossip交换出去，从而所有节点可以迅速知道整个拓扑的节点。为了避免在gossip通讯过程中的发生问题（什么问题？？文中没讲），在一个cluster中使用同一份seed节点的列表。每次节点重启，它都会记住之前已经gossip下来的其他节点信息。seed节点只是用于引导gossip进程，为新加入cluster的节点提供服务。

注意：在多个dc的cluster中，每个DC（repliaction group）至少有一个节点包含在seed列表里，否则的话，gossip不得不在引导一个新节点时与另外一个DC通讯。为了减少维护成本和保持gossip的性能，不要把每个节点都设置为seed节点，推荐配置一个小的seed列表（每个DC 3个seed节点）。

Cassandra使用gossip进行故障检测，从而避免客户端的请求发送到不可达的节点或者性能很差的节点。gossip进程通过直连或者间接的信息跟踪节点的信息。在gossip通讯过程中，每个节点维护一个滑动窗口保存其他节点发来信息的次数，通过配置phi_convict_threshold属性来让gossip判定一个节点是否挂掉（应该是一段时间内没收到某节点的信息，就认为它挂掉），这个属性的值越低，那么就越轻易判定一个节点死刑。在Amazon EC2的环境里，一般把这个值设置高一点，因为EC2的网络比较差，不要太轻易认定一个节点挂了。这个值不推荐设置大于12或者小于5.

当一个节点重新上线，它可能已经丢失了它本该维护很多replica数据。一旦故障检测把这个节点标记为down，那么那些enable了‘hinted handoff’的节点就会帮它保存那些miss的replica一段时间，如果一个节点挂掉超过max_hint_window_in_ms（默认3小时）时间，那么就不再保存那些hints，此时需要运行nodetool repair来恢复数据，保持所有节点拥有一致性数据。

## 数据分布和复制

在cassandra里，数据的分布和复制一起进行。数据是以表的形式组织，并以一个主键唯一标识，主键决定了数据在哪里存储。复制品是行的靠背，当数据第一次被写，它也本身也是可以看做是一个复制品。

影响复制的因子包含：

* Virtual nodes: 向物理机器分派数据
* Partitioner: 在cluster中划分数据
* Replica stratege: determines the replicas for each row of data. ???
* Snitch:　定义复制策略使用的拓扑信息


### 一致性哈希

一致性哈希可减少节点增加或移除带来的数据重组织的成本（why?）.一致性哈希分区基于partition key，例如有以下数据：
![table]({{ site.baseurl }}/img/cassandra01/exampl1.png)

cassandra为每个partition key计算哈希值：
![hash]({{ site.baseurl }}/img/cassandra01/hash.png)

每个节点将负责保存某一段哈希值范围的数据：
![hash_ring]({{ site.baseurl }}/img/cassandra01/hash_ring.png)

cassandra将根据partition key计算出来的哈希值，找到那个包含该哈希值范围的节点：
![hash_node]({{ site.baseurl }}/img/cassandra01/hash_node.png)

### Virtual nodes
Vnodes可以简化cassandra的很多任务：

* 你不需要为每个节点计算并赋予tokens
* 当新增或删除节点时，不需要rebalance cluster。
* 重建一个挂掉的节点更快
* 改善cluster里异构节点的利用率。

看不懂？好吧，我也看不懂，暂且继续往下看：

在1.2之前，你不得不为每个节点计算并赋予一个token。每个token决定了节点在哈希环中的位置。从1.2开始，cassandra允许每个节点可以有多个token。这个新的范式叫做virtual nodes（vnodes）。Vnodes允许每个节点拥有大量的小的partition范围。Vnodes也是使用一致性哈希来分派数据，但是不需要生成token。

![vnode]({{ site.baseurl }}/img/cassandra01/vnodes.png)

在上图的上部分是没有vnodes的cluster，在这个范式里，每个节点赋予一个token，代表了在环中的位置。每个节点存储什么数据是由数据的parition key决定，每个token代表了一个哈希值的范围，那么由该数据的partition key计算出来的哈希值在此token的所代表的哈希值的范围内，则把数据存储到这个节点上。每个节点还会包含其他节点的数据的复制品。例如，范围E被复制到节点5/6/1.注意，每一个节点拥有一个连续的在环空间里的partition范围。

在图的下部分是vnodes的环。在cluster里，virtual nodes是随机选择并且非连续的。每个节点的partition范围也变得很小。

### Data replication

cassendra把数据的复制品分散在不同的几点上以保证高可用性。replication 策略决定了复制品放到哪些节点上。replication factor指定了数据有多少个复制品。如果replication factor=1, 那么数据将只有一份复制品。加入是2，那么每一行数据将有2个复制品并分散在不同的节点上。每一个复制品都是相同重要的，没有主次之分。一般要求replication factor小于节点的总数。

有两种可用的replication策略：

* SimpleStrategy

	仅在只有一个data center时使用。SimpleStrategy把第一个复制品放到由partitioner计算出来的节点上，额外的复制品将放置到环上顺时针的第二个节点上，但它不会考虑节点的在拓扑中的位置。
* NetworkTopologyStrategy

	当cluster拥有多个DC时，使用此策略。这个策略需要配置每个DC拥有多少份复制品。
	NetworkTopologyStrategy会顺时针地游走环，当遇到第一个在不同机架的节点时，就会把复制品放到该节点上，因为在同一个机架上的节点很有可能会同时发生故障。

## Partitioners

Partitioner决定了数据是在cluster里如何分布。一般地，partitioner是一个依据partition key衍生（deriving）token的方法，这个方法一般是哈希。然后数据将会根据这个token的值分布到节点上。

Murmur3Partitioner和RandomPartitioner都是使用token来把数据均匀的分派到各个节点上。

cassandra提供了以下3钟pertitioners:

* Murmur3Partitioner（默认）：提供更快的哈希方法，比其他partitioner性能更好。
* RandomPartitioner：在1.2之前是默认的partitioner，为了向后兼容而保留它。使用MD5哈希。
* ByteOrderedPartitioner：也是为了向后兼容才保留。它会根据key的字节来进行字典排序。

如果不适用vnodes，你必须自己计算tokens，使用Murmur3Partitioner时，计算token的方法如下：

Murmur3Partitioner的哈希值范围是： -2^63 to +2^63-1 ,运行以下命令计算tokens:

		python -c 'print [str(((2**64 / number_of_tokens) * i) - 2**63) for i in range(number_of_tokens)]'

例如，有6个节点，则有需要6个token:

		python -c 'print [str(((2**64 / number_of_tokens) * i) - 2**63) for i in range(number_of_tokens)]'

该命令结果如下：
		
		[ '-9223372036854775808', '-6148914691236517206', '-3074457345618258604', '-2', '3074457345618258600', '6148914691236517202' ]


## Snitches

Snitch决定了节点属于哪个DC和racks(机架)。Snitch告诉Cassandra网络的拓扑以便让请求更有效的分发到相应的节点上。通过snitch, cassendra尽量保证不会把同一个复制品放到同一个机架上。

* Dynamic snitching： 默认情况下，所有snitches使用一个dynamic snitch层去监控读的延时，并把请求路由到性能好的节点去。dynamic snitch是默认启用的，并且在多数部署情况下推荐使用。
* SimpleSnitch:  仅在只有单个DC的情况下使用.
* RackInferringSnitch: 根据节点的IP地址来识别data center和rack的位置
* PropertyFileSnitch：使用cassandra-topology.properties文件配置节点的IP
* GossipingPropertyFileSnitch：这是生产环境推荐使用的snitch，每个节点使用cassandra-rackdc.properties配置它的本地的data center和rack的信息，并通过gossip收集其他节点的信息。


## Client Request

客户端可以把请求发送到任意一个节点上，因为在cassandra中，所有节点都是等同的。当客户端连接上一个node并发出读写请求，这个节点将会作为一个代理（根据partitioner和replica placement 策略配置）一样把请求发送到真正能处理那个请求的节点上。



